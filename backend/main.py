import asyncio
import json
from datetime import datetime
import io
import base64
import uuid
import logging
import os
from typing import Dict, Any
from queue import Queue
import queue
import threading
from pydantic import BaseModel
import openai 

import torch
from PIL import Image
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

from pymongo import MongoClient

# --- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---
import config
import model_inference
from feedback_generator import GenerateFeedback
from data_process import analyze_concentration_changes
from wav_process import FastAudioPreprocessor, preprocess_audio_data
from merge_wav import merge_wav_chunks_from_buffer as merge
from llm_prompt import LLMPipeline

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- FastAPI ì•± ì´ˆê¸°í™” ---
app = FastAPI()

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì • ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œ ì¤‘ì—ëŠ” ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì†Œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

# ìƒì„±ëœ ê·¸ë˜í”„ ì´ë¯¸ì§€ ë“±ì„ ì œê³µí•˜ê¸° ìœ„í•œ ì •ì  íŒŒì¼ ê²½ë¡œ ë§ˆìš´íŠ¸
app.mount("/static", StaticFiles(directory=config.STATIC_DIR), name="static")

client = MongoClient("mongodb://localhost:27017")
db = client["mydatabase"]
collection = db["sessions"]

audio_conf = {
    'num_mel_bins': 128, 
    'target_length': 1024,
    'dataset': 'aihub_audio_dataset', 
    'mean':-4.2677393, 
    'std':4.5689974
    }

def save_result(sessionid, result):
    collection.update_one(
        {"session_id": sessionid},
        {"$push": {"results": result}},
        upsert=True
    )

# --- ì„¸ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ---
class SessionManager:
    """ì›¹ì†Œì¼“ í´ë¼ì´ì–¸íŠ¸ë³„ ì„¸ì…˜ì„ ê´€ë¦¬"""

    def __init__(self):
        self.sessions: Dict[str, Dict[str, Any]] = {}

    def create_session(self, user_name: str, topic: str) -> str:
        session_id = str(uuid.uuid4())
        self.sessions[session_id] = {
            "user_name": user_name,
            "topic": topic,
            "start_time": datetime.now(),
        }
        logger.info(f"ì„¸ì…˜ ìƒì„±ë¨: {session_id} (ì‚¬ìš©ì: {user_name})")
        return session_id

    def get_session(self, session_id: str) -> Dict[str, Any] | None:
        return self.sessions.get(session_id)

    def remove_session(self, session_id: str):
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.info(f"ì„¸ì…˜ ì¢…ë£Œë¨: {session_id}")

session_manager = SessionManager()

class ChatRequest(BaseModel):
    session_id: str
    message: str
    user_name: str = "í•™ìƒ"
    topic: str = "ê²½ì˜ì •ë³´ì‹œìŠ¤í…œ"
    
# --- í•µì‹¬ ë¶„ì„ê¸° í´ë˜ìŠ¤ ---
class LectureAnalyzer:
    """ëª¨ë“  ëª¨ë¸ì„ ì´ê´„í•˜ê³  ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰"""

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if not config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        self.openai_client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
        
        logger.info(f"ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘... (Device: {self.device})")
        try:
            self.pad = FastAudioPreprocessor(audio_conf)
            logger.info("âœ… ìŒì„± ì „ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
            self.face_box_model, self.e2e_model = model_inference.load_model()
            self.e2e_model.to(self.device)
            model_inference.warmup_model(self.face_box_model, self.e2e_model)
            logger.info("âœ… ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.critical(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", exc_info=True)
            raise RuntimeError("í•„ìˆ˜ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í•˜ì—¬ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from e

    def _transcribe_audio(self, audio_path: str) -> str:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisper APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            with open(audio_path, "rb") as audio_file:
                transcription = self.openai_client.audio.transcriptions.create(
                    model="whisper-1", 
                    file=audio_file
                )
            logger.info(f"Whisper ë³€í™˜ ì™„ë£Œ: {transcription.text}")
            return transcription.text
        except Exception as e:
            logger.error(f"Whisper API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def process_chunk(
        self, frame_data: bytes, audio_path: str, last_timestamp: float
    ) -> Dict | None:
        """ì‹¤ì‹œê°„ ë°ì´í„° ì²­í¬ë¥¼ ë°›ì•„ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ë¶„ì„"""
        try:
            pil_image = Image.open(io.BytesIO(frame_data))
            pil_image.save("output.jpg")
            audio_tensor = preprocess_audio_data(self.pad, audio_path)
            ((pred_num, pred_str), (yaw, pitch), (noise_num, noise_str)), audio = (
                model_inference.run(
                    self.face_box_model, self.e2e_model, pil_image, audio_tensor
                )
            )
            transcribed_text = self._transcribe_audio(audio_path)
            start_time = last_timestamp - config.TIMESTEP
            result = {
                "timestamp": {"start": start_time, "end": last_timestamp},
                "result": {"num": pred_num, "str": pred_str},
                "pose": {"yaw": float(yaw), "pitch": float(pitch)},
                "noise": {"num": noise_num, "str": noise_str},
                "text": transcribed_text,
            }
            return result
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return None

    def generate_final_report(self, session_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        user_name = session_data["user_name"]
        topic = session_data["topic"]
        logger.info(f"'{user_name}'ë‹˜ì˜ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        try:
            feedback_generator = GenerateFeedback()
            doc = collection.find_one({"session_id": session_id}, {"_id": 0, "results": 1})
            results = doc.get("results", []) if doc else []
            if not results:
                logger.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {"error": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±"}
            full_html_report = feedback_generator.generate(
                topic=topic, name=user_name, data=results
            )
            logger.info("âœ… LLM ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            safe_user_name = "".join(c for c in user_name if c.isalnum())
            report_filename = f"feedback_{safe_user_name}_{uuid.uuid4().hex[:6]}.html"
            report_path_abs = os.path.join(config.STATIC_DIR, report_filename)
            with open(report_path_abs, "w", encoding="utf-8") as f:
                f.write(full_html_report)
            logger.info(f"âœ… HTML ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {report_path_abs}")
            collection.update_one(
                {"session_id": session_id},
                {"$set": {"final_report_html": full_html_report}}
            )
            logger.info(f"âœ… ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            sorted_results = sorted(results, key=lambda x: x["timestamp"]["start"])
            insights = analyze_concentration_changes(sorted_results)
            logger.info("âœ… ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ")
            return {
                "user_name": user_name,
                "topic": topic,
                "llm_report": full_html_report,
                "detailed_analysis": insights,
            }
        except Exception as e:
            logger.error(f"ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return {"error": "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ"}

# --- ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
try:
    analyzer = LectureAnalyzer()
    llm_pipeline = LLMPipeline()
    logger.info("âœ… LLM Pipeline ë¡œë“œ ì™„ë£Œ")
except RuntimeError as e:
    logger.critical(f"ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨. ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    analyzer = None

# class SessionAudioBuffer:
#     def __init__(self, session_id: str, analyzer):
#         self.session_id = session_id
#         self.analyzer = analyzer
#         self.num_chunks = 0
#         self._shutdown = False
#         self.buffer = b''
#         self.frame_latest = None
#         self.model_queue = Queue()
#         self.model_thread = threading.Thread(target=self._model_worker)
#         self.model_thread.start()

#     def _model_worker(self):
#         """ëª¨ë¸ ì¶”ë¡  ì „ìš© ì›Œì»¤ ìŠ¤ë ˆë“œ. íì— ë“¤ì–´ì˜¨ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
#         while True:
#             task = self.model_queue.get()
#             if task is None:  # ì¢…ë£Œ ì‹ í˜¸
#                 self.model_queue.task_done()
#                 break
            
#             wav_path = ""
#             try:
#                 wav_path = self._save_wav_file(task)
#                 result = self.analyzer.process_chunk(
#                     task['frame'], wav_path, task['timestamp']
#                 )
#                 if result:
#                     save_result(self.session_id, result)
#             except Exception as e:
#                 logger.error(f"ëª¨ë¸ ì›Œì»¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Session {self.session_id}): {e}", exc_info=True)
#             finally:
#                 self._cleanup_wav_file(wav_path)
#                 self.model_queue.task_done()
class SessionAudioBuffer:
    def __init__(self, session_id: str, analyzer):
        self.session_id = session_id
        self.analyzer = analyzer
        self.num_chunks = 0
        self._shutdown = False
        self.buffer = b''
        self.frame_latest = None
        self.model_queue = Queue()
        
        # ë°°ì¹˜ ì²˜ë¦¬ë§Œ ì‚¬ìš©
        self.batch_size = 3
        self.batch_buffer = []
        
        self.model_thread = threading.Thread(target=self._model_worker)
        self.model_thread.start()

    def _model_worker(self):
        """ë°°ì¹˜ ì²˜ë¦¬ë§Œ í•˜ëŠ” ì›Œì»¤ (ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ì œê±°)"""
        while True:
            try:
                task = self.model_queue.get(timeout=1)
                if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                    if self.batch_buffer:
                        asyncio.run(self._process_batch())
                    break
                
                # ë°°ì¹˜ì— ì¶”ê°€
                self.batch_buffer.append(task)
                
                # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ shutdownì´ë©´ ì²˜ë¦¬
                if len(self.batch_buffer) >= self.batch_size or self._shutdown:
                    asyncio.run(self._process_batch())
                
                self.model_queue.task_done()
                
            except queue.Empty:
                if self._shutdown and self.batch_buffer:
                    asyncio.run(self._process_batch())
                continue

    async def _process_batch(self):
        """ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬"""
        if not self.batch_buffer:
            return
            
        current_batch = self.batch_buffer.copy()
        self.batch_buffer.clear()
        
        logger.info(f"Session {self.session_id}: ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ({len(current_batch)}ê°œ)")
        
        # ë°°ì¹˜ ë‚´ ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        batch_tasks = []
        for task in current_batch:
            task_coroutine = self._process_single_task_async(task)
            batch_tasks.append(task_coroutine)
        
        # ìˆœì„œ ë³´ì¥ ë³‘ë ¬ ì²˜ë¦¬
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì €ì¥
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                logger.error(f"ë°°ì¹˜ ì˜¤ë¥˜: {result}")
            elif result:
                save_result(self.session_id, result)
                
        logger.info(f"Session {self.session_id}: ë°°ì¹˜ ì™„ë£Œ")

    async def _process_single_task_async(self, task):
        """ê°œë³„ ì‘ì—… ë¹„ë™ê¸° ì²˜ë¦¬"""
        wav_path = ""
        try:
            wav_path = self._save_wav_file(task)
            
            # ë¹„ë™ê¸° ì²­í¬ ì²˜ë¦¬
            result = await self._process_chunk_async(
                task['frame'], wav_path, task['timestamp']
            )
            return result
            
        except Exception as e:
            logger.error(f"ê°œë³„ ì‘ì—… ì˜¤ë¥˜: {e}")
            return None
        finally:
            self._cleanup_wav_file(wav_path)

    async def _process_chunk_async(self, frame_data, audio_path, timestamp):
        """ë¹„ë™ê¸° ì²­í¬ ì²˜ë¦¬ (STTë§Œ ë¹„ë™ê¸°, ë‚˜ë¨¸ì§€ëŠ” ë™ê¸°)"""
        try:
            # ë™ê¸° ì²˜ë¦¬ ë¶€ë¶„
            pil_image = Image.open(io.BytesIO(frame_data))
            pil_image.save("output.jpg")
            audio_tensor = preprocess_audio_data(self.analyzer.pad, audio_path)
            ((pred_num, pred_str), (yaw, pitch), (noise_num, noise_str)), audio = (
                model_inference.run(
                    self.analyzer.face_box_model, 
                    self.analyzer.e2e_model, 
                    pil_image, 
                    audio_tensor
                )
            )
            
            # ë¹„ë™ê¸° STT ì²˜ë¦¬ - í•µì‹¬!
            transcribed_text = await self._async_whisper_call(audio_path)
            
            start_time = timestamp - config.TIMESTEP
            result = {
                "timestamp": {"start": start_time, "end": timestamp},
                "result": {"num": pred_num, "str": pred_str},
                "pose": {"yaw": float(yaw), "pitch": float(pitch)},
                "noise": {"num": noise_num, "str": noise_str},
                "text": transcribed_text,
            }
            return result
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None

    async def _async_whisper_call(self, audio_path: str) -> str:
        """Whisper API ë¹„ë™ê¸° í˜¸ì¶œ"""
        def whisper_sync_call():
            try:
                with open(audio_path, "rb") as audio_file:
                    transcription = self.analyzer.openai_client.audio.transcriptions.create(
                        model="whisper-1", 
                        file=audio_file,
                        timeout=10
                    )
                return transcription.text
            except Exception as e:
                logger.warning(f"Whisper ì˜¤ë¥˜: {e}")
                return "[ìŒì„±ì¸ì‹ ì‹¤íŒ¨]"
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=3) as executor:
            text = await loop.run_in_executor(executor, whisper_sync_call)
        return text
    def _save_wav_file(self, task):
        wav_path = os.path.join(
            config.TEMP_DIR_PATH, 
            f"audio_{task['timestamp']:03d}_{self.session_id}.wav"
        )
        os.makedirs(config.TEMP_DIR_PATH, exist_ok=True)
        merged_wav = merge(task['audio_bytes'])
        with open(wav_path, "wb") as f:
            f.write(merged_wav)
        return wav_path

    def _cleanup_wav_file(self, wav_path):
        try:
            if os.path.exists(wav_path):
                os.remove(wav_path)
        except Exception as e:
            logger.warning(f"WAV íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {wav_path}: {e}")

    async def add_chunk(self, audio_b64: str, frame_b64: str):
        if self._shutdown:
            return
        try:
            self.buffer += base64.b64decode(audio_b64)
            if frame_b64:
                self.frame_latest = base64.b64decode(frame_b64)
            self.num_chunks += 1
            if self.should_process():
                self._enqueue_for_processing()
        except Exception as e:
            logger.error(f"ì²­í¬ ì¶”ê°€ ì˜¤ë¥˜ (Session {self.session_id}): {e}")

    def _enqueue_for_processing(self):
        try:
            processing_data = {
                "audio_bytes": self.buffer,
                "frame": self.frame_latest,
                "timestamp": int(self.num_chunks),
                "session_id": self.session_id
            }
            self.model_queue.put(processing_data)
            self.buffer = b''
            self.frame_latest = None
        except Exception as e:
            logger.error(f"ë°ì´í„° í ì¶”ê°€ ì˜¤ë¥˜: {e}")

    def should_process(self) -> bool:
        return self.num_chunks % config.TIMESTEP == 0

    def shutdown(self):
        """ëª¨ë“  í ì‘ì—…ì„ ì™„ë£Œí•˜ê³  ìŠ¤ë ˆë“œë¥¼ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        logger.info(f"Session {self.session_id}: Shutdown initiated.")
        self._shutdown = True
        if self.buffer:
            self._enqueue_for_processing()
        
        logger.info(f"Session {self.session_id}: Waiting for all tasks to be processed...")
        self.model_queue.join()  # íì˜ ëª¨ë“  í•­ëª©ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ë¸”ë¡œí‚¹
        
        logger.info(f"Session {self.session_id}: All tasks processed. Stopping worker thread.")
        self.model_queue.put(None) # ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        self.model_thread.join()
        logger.info(f"Session {self.session_id}: Shutdown complete.")

# ì „ì—­ ì„¸ì…˜ ê´€ë¦¬
session_buffers = {}

# --- ì›¹ì†Œì¼“ ì—”ë“œí¬ì¸íŠ¸ ---
@app.websocket("/ws/lecture-analysis")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    session_id = None
    if not analyzer:
        await websocket.send_json({"type": "error", "message": "ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨."})
        await websocket.close(code=1011)
        return

    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            msg_type = message.get("type")

            if msg_type == "start_session":
                user_name = message.get("user_name", "í•™ìƒ")
                topic = message.get("topic", "í•™ìŠµ ì£¼ì œ")
                session_id = session_manager.create_session(user_name, topic)
                session_buffers[session_id] = SessionAudioBuffer(session_id, analyzer)
                await websocket.send_json({"type": "session_started", "session_id": session_id})

            elif msg_type == "data_chunk":
                if session_id and session_id in session_buffers:
                    await session_buffers[session_id].add_chunk(
                        message.get("audio"), message.get("frame")
                    )
                    # ì‹¤ì‹œê°„ í”¼ë“œë°±ì„ ë°°ì¹˜ í¬ê¸°ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
                    session = session_manager.get_session(session_id)
                    if session:
                        if "feedback_counter" not in session:
                            session["feedback_counter"] = 0
                        session["feedback_counter"] += 1

                        # ğŸ”¥ ë°°ì¹˜ í¬ê¸°(3) * 2 = 6ì²­í¬ë§ˆë‹¤ í”¼ë“œë°± (ë°°ì¹˜ ì™„ë£Œ í›„ ì¡°íšŒ)
                        if session["feedback_counter"] % 6 == 0:
                            # ì ì‹œ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì¡°íšŒ (ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°)
                            await asyncio.sleep(1)  
                            
                            doc = collection.find_one({"session_id": session_id}, {"_id": 0, "results": 1})
                            results = doc.get("results", []) if doc else []
                            if results:
                                await websocket.send_json({
                                    "type": "realtime_feedback",
                                    "concentration": results[-1]['result']['str'],
                                    "noise": results[-1]['noise']['str'],
                                })
                    # # --- ì‹¤ì‹œê°„ í”¼ë“œë°± ì „ì†¡ ë¡œì§ ---
                    # session = session_manager.get_session(session_id)
                    # if session:
                    #     if "feedback_counter" not in session:
                    #         session["feedback_counter"] = 0
                    #     session["feedback_counter"] += 1

                    #     if session["feedback_counter"] % 5 == 0: # 5 ì²­í¬ë§ˆë‹¤ í”¼ë“œë°± ì „ì†¡
                    #         doc = collection.find_one({"session_id": session_id}, {"_id": 0, "results": 1})
                    #         results = doc.get("results", []) if doc else []
                    #         if results:
                    #             await websocket.send_json(
                    #                 {
                    #                     "type": "realtime_feedback",
                    #                     "concentration": results[-1]['result']['str'],
                    #                     "noise": results[-1]['noise']['str'],
                    #                 }
                    #             )

            elif msg_type == "end_session":
                if not session_id:
                    await websocket.send_json({"type": "error", "message": "ì„¸ì…˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."})
                    break
                
                if session_id in session_buffers:
                    buffer = session_buffers[session_id]
                    await websocket.send_json({"type": "status_update", "message": "Final audio processing..."})
                    await asyncio.to_thread(buffer.shutdown)
                    del session_buffers[session_id]

                session = session_manager.get_session(session_id)
                if session:
                    logger.info(f"ì„¸ì…˜ '{session_id}'ì˜ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
                    await websocket.send_json({"type": "report_generating", "message": "ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."})
                    try:
                        final_report = await asyncio.to_thread(
                            analyzer.generate_final_report, session, session_id
                        )
                        await websocket.send_json({"type": "final_report", "data": final_report})
                    except Exception as e:
                        await websocket.send_json({"type": "error", "message": f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"})
                
                session_manager.remove_session(session_id)
                break
            
    except WebSocketDisconnect:
        logger.info(f"ì›¹ì†Œì¼“ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤: {session_id}")
    finally:
        if session_id and session_id in session_buffers:
            session_buffers[session_id].shutdown()
            del session_buffers[session_id]
        if session_id and session_manager.get_session(session_id):
            session_manager.remove_session(session_id)

@app.post("/api/chat")
async def chat_with_teacher(request: ChatRequest):
    try:
        doc = collection.find_one({"session_id": request.session_id}, {"_id": 0, "results": 1, "final_report_html": 1})
        if not doc:
            return {"success": False, "response": "ì„¸ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        results = doc.get("results", [])
        final_report = doc.get("final_report_html", "")
        ai_response = llm_pipeline.generate_chat_response(
            user_message=request.message,
            user_name=request.user_name,
            topic=request.topic,
            analysis_results=results,
            final_report=final_report
        )
        return {"success": True, "response": ai_response}
    except Exception as e:
        logger.error(f"ì±„íŒ… API ì˜¤ë¥˜: {e}")
        return {"success": False, "response": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@app.get("/health")
def health_check():
    return {"status": "healthy" if analyzer else "unhealthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)